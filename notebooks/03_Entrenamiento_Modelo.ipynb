{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# ------------------- CARGAR DATOS PROCESADOS -------------------\n",
    "df = pd.read_csv(\"../data/processed/dataset_clean.csv\")\n",
    "\n",
    "# Definir características y variable objetivo\n",
    "features = ['duration_ms', 'explicit', 'danceability', 'energy', 'key', \n",
    "            'loudness', 'mode', 'speechiness', 'acousticness', \n",
    "            'instrumentalness', 'liveness', 'valence', 'tempo', \n",
    "            'time_signature', 'track_genre_encoded']\n",
    "\n",
    "X = df[features]\n",
    "y = df['popularity']\n",
    "\n",
    "# Normalización de datos\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# División en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ------------------- ENTRENAMIENTO Y GUARDADO DE MODELOS -------------------\n",
    "# Lista de modelos a probar\n",
    "modelos = {\n",
    "    \"Regresión Lineal\": LinearRegression(),\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(random_state=42),\n",
    "    \"SVR\": SVR()\n",
    "}\n",
    "\n",
    "# Carpeta para guardar modelos\n",
    "models_dir = \"../models/\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Evaluar cada modelo y guardar sus resultados\n",
    "mejor_modelo = None\n",
    "mejor_rmse = float(\"inf\")\n",
    "\n",
    "print(\"\\nEntrenando modelos...\")\n",
    "for nombre, modelo in modelos.items():\n",
    "    modelo.fit(X_train, y_train)\n",
    "    predicciones = modelo.predict(X_test)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predicciones))\n",
    "    mae = mean_absolute_error(y_test, predicciones)\n",
    "    r2 = r2_score(y_test, predicciones)\n",
    "\n",
    "    print(nombre, \"- RMSE:\", round(rmse, 1), \"- MAE:\", round(mae, 1), \"- R²:\", round(r2, 3))\n",
    "\n",
    "    # Guardar modelo en la carpeta \"models\"\n",
    "    modelo_path = os.path.join(models_dir, f\"{nombre.replace(' ', '_')}.pkl\")\n",
    "    joblib.dump(modelo, modelo_path)\n",
    "\n",
    "    # Actualizar el mejor modelo según RMSE\n",
    "    if rmse < mejor_rmse:\n",
    "        mejor_rmse = rmse\n",
    "        mejor_modelo = modelo_path\n",
    "\n",
    "print(\"\\nMejor modelo seleccionado:\", mejor_modelo)\n",
    "\n",
    "# ------------------- OPTIMIZACIÓN DEL MEJOR MODELO -------------------\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'bootstrap': [True, False],\n",
    "    'criterion': ['squared_error', 'absolute_error']\n",
    "}\n",
    "\n",
    "# Cargar el mejor modelo encontrado\n",
    "modelo_final = joblib.load(mejor_modelo)\n",
    "\n",
    "# Optimizar el modelo con RandomizedSearchCV\n",
    "search = RandomizedSearchCV(modelo_final, param_dist, n_iter=10, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, random_state=42)\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nMejor modelo optimizado:\", search.best_params_)\n",
    "print(\"Error optimizado (RMSE):\", round(np.sqrt(-search.best_score_), 1))\n",
    "\n",
    "# Guardar el modelo final\n",
    "modelo_final_path = os.path.join(models_dir, \"modelo_final.pkl\")\n",
    "joblib.dump(search.best_estimator_, modelo_final_path)\n",
    "print(\"Modelo final guardado en:\", modelo_final_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
